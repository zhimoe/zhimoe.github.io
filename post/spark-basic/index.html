<!doctype html><html lang=zh dir=zh><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=content-security-policy content="upgrade-insecure-requests"><title>Spark Basic - zhimoe</title><meta name=keywords content="读书,代码,架构,JVM,Java,Kotlin,Python,Rust"><meta name=author content="zhimoe"><meta property="og:title" content="Spark Basic"><meta property="og:site_name" content="zhimoe"><meta property="og:image" content="/img/author.jpg"><meta name=title content="Spark Basic - zhimoe"><meta name=description content="编程是一门手艺"><link rel="shortcut icon" href=https://zhimoe.github.io/img/favicon.ico><link rel=apple-touch-icon href=https://zhimoe.github.io/img/apple-touch-icon.png><link rel=apple-touch-icon-precomposed href=https://zhimoe.github.io/img/apple-touch-icon.png><link href=https://cdn.jsdelivr.net/npm/typeface-fira-sans@1.1.13/index.min.css rel=stylesheet type=text/css><link href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css rel=stylesheet type=text/css><link href=https://cdn.jsdelivr.net/npm/imageviewer@1.1.0/dist/viewer.min.css rel=stylesheet><link href=https://zhimoe.github.io/css/main.css rel=stylesheet type=text/css><link href=https://zhimoe.github.io/css/syntax.css rel=stylesheet type=text/css><script async src="https://www.googletagmanager.com/gtag/js?id=G-CJJZFN2DZR"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-CJJZFN2DZR")</script><script>const apiEndpoint="https://pageview.zhimoe.workers.dev/write",payload={url:document.location.href,title:document.title,referrer:window.frames.top.document.referrer};fetch(`${apiEndpoint}?${new URLSearchParams(payload)}`,{mode:"no-cors"}).catch(console.log)</script></head><body itemscope itemtype=http://schema.org/WebPage lang=zh-hans><div class="container one-collumn sidebar-position-left page-home"><div class=headband></div><header id=header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle role=button style=opacity:1;top:0><span class=toggle-line></span>
<span class=toggle-line></span>
<span class=toggle-line></span></div></div><div class="site-meta custom-logo"><div class=custom-logo-site-title><a href=https://zhimoe.github.io/ class=brand rel=start><span class=logo-line-before><i></i></span>
<span class=site-title>zhimoe</span>
<span class=logo-line-after><i></i></span></a></div><p class=site-subtitle>Captain your own Ship.</p></div><div class=site-nav-right><div class="toggle popup-trigger" style=opacity:1;top:0><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul id=menu class=menu><li class=menu-item><a href=https://zhimoe.github.io/ rel=section><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class=menu-item><a href=https://zhimoe.github.io/post/ rel=section><i class="menu-item-icon fa fa-fw fa-list-alt"></i><br>归档</a></li><li class=menu-item><a href=https://zhimoe.github.io/categories/%E7%BC%96%E7%A8%8B/ rel=section><i class="menu-item-icon fa fa-fw fa-wpexplorer"></i><br>编程</a></li><li class=menu-item><a href=https://zhimoe.github.io/categories/%E7%BF%BB%E8%AF%91/ rel=section><i class="menu-item-icon fa fa-fw fa-language"></i><br>翻译</a></li><li class=menu-item><a href=https://zhimoe.github.io/categories/%E9%A1%B9%E7%9B%AE/ rel=section><i class="menu-item-icon fa fa-fw fa-bathtub"></i><br>项目</a></li><li class=menu-item><a href=https://zhimoe.github.io/about/ rel=section><i class="menu-item-icon fa fa-fw fa-grav"></i><br>关于</a></li><li class="menu-item menu-item-search"><a href=javascript:; class=popup-trigger><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon><i class="fa fa-search"></i></span>
<span class=popup-btn-close><i class="fa fa-times-circle"></i></span><div class=local-search-input-wrapper><input autocomplete=off placeholder=搜索关键字... spellcheck=false type=text id=local-search-input autocapitalize=none autocorrect=off></div></div><div id=local-search-result></div></div></div></nav></div></header><main id=main class=main><div class=main-inner><div class=content-wrap><div id=content class=content><section id=posts class=posts-expand><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline"><a class=post-title-link href=https://zhimoe.github.io/post/spark-basic/ itemprop=url>Spark Basic</a></h1><div class=post-meta><span class=post-time><span class=post-meta-item-text>时间：</span>
<time itemprop=dateCreated datetime=2016-03-22T13:04:35+08:00 content="2018-03-31">2018-03-31</time></span>
<span class=post-category>&nbsp; | &nbsp;
<span class=post-meta-item-text>分类：</span>
<span itemprop=about itemscope itemtype=https://schema.org/Thing><a href=https://zhimoe.github.io/categories/%E7%BC%96%E7%A8%8B itemprop=url rel=index style=text-decoration:underline><span itemprop=name>编程</span></a>
&nbsp;</span></span>
<span>|
<span class=post-meta-item-text>字数：</span>
<span class=leancloud-world-count>3896 字</span></span>
<span>|
<span class=post-meta-item-text>阅读：</span>
<span class=leancloud-view-count>8分钟</span></span></div></header><div class=post-body itemprop=articleBody><h2 id=引言>引言</h2><p>大数据计算和普通的程序并无本质区别：数据输入=>计算=>输出和结果的持久化.这里的挑战在于计算的效率和容错性.由于数据输入巨大,计算的效率是基本的要求.为了在通用硬件上高效完成大量计算,唯一的途径就是将计算任务拆分分布式计算.这就引出了新的问题：分布式计算资源的管理（Mesos,YARN）,分布式计算失败后的恢复（容错性）（Spark RDD）,以及分布式的数据输入和保存（分布式文件HDFS）.hadoop生态圈就是为了解决几个问题设计的(YARN,MapR,HDFS).只不过在计算这一环节Spark做的更加高效取代了MapR.所以先看下hadoop的核心两个组件.</p><h2 id=hdfs>HDFS</h2><ul><li>HDFS是hadoop的虚拟分布式文件系统.满足大数据问题下要求的：可扩展的,容错的,硬件通用的和高并发的特性.HDFS最重要的特性是不可变性&ndash;数据提交到HDFS后即不可更新了,也就是所谓的WORM(write once read many).</li><li>文件在HDFS中是以block构成,默认一个block是128M.block是是分布式的,即如果集群中如果有多于1个节点,那么有文件可能会被分布在多个节点上.block是被复制的,这主要是两个目的：1.容错,2.增加数据局部性的概率,有利于访问.block复制在数据节点接收（ingest：消化）block时同时发生.如图所示：</li></ul><p><img src=https://zhimoe.github.io/spark/File-ingestion-into-a-multinode-cluster.png alt="File ingestion into a multinode cluster"></p><ul><li>NameNode：不知道怎么翻译,NameNode主要负责管理HDFS的元数据,包括directory,文件对象和相关属性（e.g. ACL),元数据是常驻内存中的,硬盘上也有备份以及日志保证持久性和崩溃后的一致性（和数据库相似）.还包括block的位置信息&ndash;block之间的关系.注意,数据（文件）并不经过NameNode,否则很容易成为性能瓶颈,数据是直接到达DataNode,并上报给NameNode管理.</li><li>数据节点（DataNode）负责：block复制；管理本节点的存储；向NameNode上报block信息.注意,数据节点不会意识到HDFS的目录（directory）和文件（Files）的概念,这些信息是NameNode管理保存的,客户端只会和NameNode交道.</li><li>hdfs客户端分为：fs shell;hdfs java api;rest proxy接口（HttpFS等）.</li><li>常见命令：</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:green># 上传一个文件 -f表示覆盖</span>
</span></span><span style=display:flex><span>hadoop fs -put -f jour.txt /user/dahu/jour/
</span></span><span style=display:flex><span><span style=color:green># 下载</span>
</span></span><span style=display:flex><span>hadoop fs -get /user/dahu/jour/jour.txt
</span></span><span style=display:flex><span><span style=color:green># ls</span>
</span></span><span style=display:flex><span>hadoop fs -ls /user/dahu/
</span></span><span style=display:flex><span><span style=color:green># 删除 -r表示递归,删除目录</span>
</span></span><span style=display:flex><span>hadoop fs -rm /user/dahu/jour/jour.txt
</span></span><span style=display:flex><span>hadoop fs -rm -r /user/dahu/jour
</span></span></code></pre></div><h2 id=yarn>YARN</h2><ul><li>YARN:Yet Another Resource Negotiator是hadoop的资源管理器.YARN有个守护进程&ndash;ResourceManager,负责全局的资源管理和任务调度,把整个集群当作计算资源池,只关注分配,不管应用,且不负责容错.YARN将application（或者叫job）分发给各个NodeManager,NodeManager是实际的worker或者worker的代理.ResourceManager主要有两个组件：Scheduler 和 ApplicationsManager. 下图是YARN的结构示意图：</li></ul><p><img src=https://zhimoe.github.io/spark/yarn_architecture.gif alt=yarn_architecture></p><ul><li><p>上图中ResourceManager负责管理和分配全局的计算资源.而NodeManager看着更复杂一些：1.用户提交一个app给RM（ResourceManager）；2.RM在资源充足的NodeManager上启动一个ApplicationMaster（也就是这个app对应的第一个container）.3.ApplicationMaster负责在所有NodeManagers中协调创建几个task container,也包括ApplicationMaster自己所在的NodeManager（上图中紫色2个和红色的4个分别表示2个app的task container和ApplicationMaster）.4. NodeManager向各个ApplicationMaster汇报task container的进展和状态.5. ApplicationMaster向RM汇报应用的进展和状态.6.RM向用户返回app的进度,状态,结果.用户一般可通过Web UI查看这些.</p></li><li><p>上面的示意图是YARN 的核心概念,Spark程序的运行结构示意图和上面的示意图相同.每个组件都可以近似一样的理解,例如,上面的Client在Spark中叫Driver程序;ResourceManager在Spark中叫Cluster Manager（为了理解方便,认为一样即可,Spark的ClusterManager目前主要有YARN,Mesos和Spark自带的三种）；NodeManager就是Spark中的Worker Node.</p></li></ul><h2 id=spark基本概念>Spark基本概念</h2><ul><li>上图中的client程序在Spark中即Driver程序.Driver就是我们编写Spark程序app的主要部分,包括<code>SparkContext</code>的创建和关闭以及计算任务（Task）的计划（Planning,包括数据数据,转换,输出,持久化等).<code>SparkContext</code>负责和Cluster Manager通信,进行资源申请,任务的分配和监控.一般认为<code>SparkContext</code>代表Driver.</li><li>ClusterManager：就是上面说的三种-Standalone,YARN,Mesos.</li><li>ＷorkerNode: 集群中运行app代码的节点,也就是上图中YARN的NodeManager节点.一个节点运行一个/多个executor.</li><li>Executor：app运行在worker节点的一个进程,进程负责执行task的planning.Spark On YARN 中这个进程叫CoarseGrainedExecutorBackend.每个进程能并行执行的task数量取决于分配给它的CPU个数了.下图是一个Spark程序集群概览图,和上图很相似.</li></ul><p><img src=https://zhimoe.github.io/spark/cluster-overview.png alt=cluster-overview></p><ul><li>仔细对比上面两个示意图,在YARN的结构示意图中,ResourceManager为程序在某个NodeManager上创建的第一个container叫ApplicationMaster,ApplicationMaster负责只是其他的task container.在Spark On YARN有两种运行模式：client和cluster模式.在cluster模式下,用户编写的driver程序运行在YARN的ApplicationMaster的内部.<br>*RDD:Spark的核心数据结构.后面详细介绍,可以简单的理解为一个Spark程序所有需要处理的数据在Spark中被抽象成一个RDD,数据需要被拆分分发到各个worker去计算,所以RDD有一个分区（Partation）概念.一般我们的数据是放在分布式文件系统上的(e.g. HDFS),可以简单理解为一个RDD包含一或多个Partation,每个Partation对应的就是HDFS的一个block.当然,Partation不是和HDFS的block绑定的,你也可以手动的对数据进行分区,即使他们只是待处理的一个本地文件或者一个小数组. 一个Partation包含一到多个Record,Record可以理解为文本中的一行,excel的一条记录或者是kafka的一条消息.</li><li>Task：RDD的一个Patation对应一个Task,Task是单个分区上最小的处理单元.</li></ul><h2 id=rdd>RDD</h2><p>pass</p><h2 id=sparkstreaming>SparkStreaming</h2><p>pass</p><h2 id=sparkstreamingkafka>SparkStreaming+Kafka</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=color:#00f>import</span> org.apache.kafka.clients.consumer.ConsumerRecord
</span></span><span style=display:flex><span><span style=color:#00f>import</span> org.apache.kafka.common.serialization.StringDeserializer
</span></span><span style=display:flex><span><span style=color:#00f>import</span> org.apache.spark.streaming.kafka010._
</span></span><span style=display:flex><span><span style=color:#00f>import</span> org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
</span></span><span style=display:flex><span><span style=color:#00f>import</span> org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>val</span> kafkaParams <span style=color:#00f>=</span> <span style=color:#2b91af>Map</span>[<span style=color:#2b91af>String</span>, <span style=color:#2b91af>Object</span>](
</span></span><span style=display:flex><span>  <span style=color:#a31515>&#34;bootstrap.servers&#34;</span> -&gt; <span style=color:#a31515>&#34;localhost:9092,anotherhost:9092&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#a31515>&#34;key.deserializer&#34;</span> -&gt; classOf[<span style=color:#2b91af>StringDeserializer</span>],
</span></span><span style=display:flex><span>  <span style=color:#a31515>&#34;value.deserializer&#34;</span> -&gt; classOf[<span style=color:#2b91af>StringDeserializer</span>],
</span></span><span style=display:flex><span>  <span style=color:#a31515>&#34;group.id&#34;</span> -&gt; <span style=color:#a31515>&#34;use_a_separate_group_id_for_each_stream&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#a31515>&#34;auto.offset.reset&#34;</span> -&gt; <span style=color:#a31515>&#34;latest&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#a31515>&#34;enable.auto.commit&#34;</span> -&gt; (<span style=color:#00f>false</span><span style=color:#00f>:</span> <span style=color:#2b91af>java.lang.Boolean</span>)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>val</span> topics <span style=color:#00f>=</span> <span style=color:#2b91af>Array</span>(<span style=color:#a31515>&#34;topicA&#34;</span>, <span style=color:#a31515>&#34;topicB&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#00f>val</span> stream <span style=color:#00f>=</span> <span style=color:#2b91af>KafkaUtils</span>.createDirectStream[<span style=color:#2b91af>String</span>, <span style=color:#2b91af>String</span>](
</span></span><span style=display:flex><span>  streamingContext,
</span></span><span style=display:flex><span>  <span style=color:#2b91af>PreferConsistent</span>,
</span></span><span style=display:flex><span>  <span style=color:#2b91af>Subscribe</span>[<span style=color:#2b91af>String</span>, <span style=color:#2b91af>String</span>](topics, kafkaParams)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>stream.map(record <span style=color:#00f>=&gt;</span> (record.key, record.value))
</span></span></code></pre></div><p>DStream的elements:record is ConsumerRecord&lt;K,V>: A key/value pair to be received from Kafka. This consists of a topic name and a partition number, from which the record is being received and an offset that points to the record in a Kafka partition.包含key(),offset(),partation()方法等.</p><ul><li>当一个StreamingContext中有多个input stream时,记得保证给程序分配了足够的资源（特别是core的数量,必须大于输入源的数量）.</li><li>本地执行程序时,不要使用“local” or “local[1]” as the master URL,streaming程序至少需要两个thread,一个接受数据,一个处理数据.直接使用local[n],n>输入源个数.</li><li>DStream 和RDD一样支持各种trans和action</li><li>DStream is batches of RDDs.</li></ul><h2 id=常见错误>常见错误</h2><h3 id=数据库mysql-redis连接的可序列化问题>数据库(mysql redis)连接的可序列化问题</h3><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>dstream.foreachRDD { rdd <span style=color:#00f>=&gt;</span>
</span></span><span style=display:flex><span>  <span style=color:#00f>val</span> connection <span style=color:#00f>=</span> createNewConnection()  <span style=color:green>// executed at the driver
</span></span></span><span style=display:flex><span><span style=color:green></span>  rdd.foreach { record <span style=color:#00f>=&gt;</span>
</span></span><span style=display:flex><span>    connection.send(record) <span style=color:green>// executed at the worker
</span></span></span><span style=display:flex><span><span style=color:green></span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span><span style=color:green>// 上面的写法会导致connection 不可序列化的错误: Task not serializable
</span></span></span><span style=display:flex><span><span style=color:green>// RDD的函数(map,foreach)会被序列化发送到worker节点执行,但是connection是和tcp连接,和机器绑定的,无法序列化
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span>dstream.foreachRDD { rdd <span style=color:#00f>=&gt;</span>
</span></span><span style=display:flex><span>  rdd.foreach { record <span style=color:#00f>=&gt;</span>  <span style=color:green>// on worker node
</span></span></span><span style=display:flex><span><span style=color:green></span>    <span style=color:#00f>val</span> connection <span style=color:#00f>=</span> createNewConnection() <span style=color:green>// 给每个record处理时新建一个连接,会导致严重的数据库连接性能问题
</span></span></span><span style=display:flex><span><span style=color:green></span>    connection.send(record)
</span></span><span style=display:flex><span>    connection.close()
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// 更好的方式是给每个partation新建一个连接
</span></span></span><span style=display:flex><span><span style=color:green></span>dstream.foreachRDD { rdd <span style=color:#00f>=&gt;</span>
</span></span><span style=display:flex><span>  rdd.foreachPartition { partitionOfRecords <span style=color:#00f>=&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:#00f>val</span> connection <span style=color:#00f>=</span> createNewConnection() 
</span></span><span style=display:flex><span>    partitionOfRecords.foreach(record <span style=color:#00f>=&gt;</span> connection.send(record))
</span></span><span style=display:flex><span>    connection.close()
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:green>// 最好的方法是维护一个静态线程池：
</span></span></span><span style=display:flex><span><span style=color:green></span>[<span style=color:#2b91af>ConnectionPool</span>](https<span style=color:#00f>:</span><span style=color:green>//github.com/RedisLabs/spark-redis/blob/master/src/main/scala/com/redislabs/provider/redis/ConnectionPool.scala)
</span></span></span><span style=display:flex><span><span style=color:green>// then use in partition
</span></span></span><span style=display:flex><span><span style=color:green></span>dstream.foreachRDD { rdd <span style=color:#00f>=&gt;</span>
</span></span><span style=display:flex><span>  rdd.foreachPartition { partitionOfRecords <span style=color:#00f>=&gt;</span>
</span></span><span style=display:flex><span>    <span style=color:green>// ConnectionPool is a static, lazily initialized pool of connections
</span></span></span><span style=display:flex><span><span style=color:green></span>    <span style=color:#00f>val</span> connection <span style=color:#00f>=</span> <span style=color:#2b91af>ConnectionPool</span>.getConnection()
</span></span><span style=display:flex><span>    partitionOfRecords.foreach(record <span style=color:#00f>=&gt;</span> connection.send(record))
</span></span><span style=display:flex><span>    <span style=color:#2b91af>ConnectionPool</span>.returnConnection(connection)  <span style=color:green>// return to the pool for future reuse
</span></span></span><span style=display:flex><span><span style=color:green></span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span><span style=color:green>// Note that the connections in the pool should be lazily created on demand and timed out if not used for a while. 
</span></span></span><span style=display:flex><span><span style=color:green>// This achieves the most efficient sending of data to external systems.
</span></span></span><span style=display:flex><span><span style=color:green></span>
</span></span><span style=display:flex><span><span style=color:green>// 示例
</span></span></span><span style=display:flex><span><span style=color:green></span><span style=color:#00f>case</span> <span style=color:#00f>class</span> <span style=color:#2b91af>RedisCluster</span>(clusterHosts<span style=color:#00f>:</span> <span style=color:#2b91af>String</span>, password<span style=color:#00f>:</span> <span style=color:#2b91af>String</span>) <span style=color:#00f>extends</span> <span style=color:#2b91af>Serializable</span> {
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#00f>def</span> <span style=color:#00f>this</span>(conf<span style=color:#00f>:</span> <span style=color:#2b91af>SparkConf</span>) {
</span></span><span style=display:flex><span>    <span style=color:#00f>this</span>(
</span></span><span style=display:flex><span>      conf.get(<span style=color:#a31515>&#34;spark.redis.host&#34;</span>, <span style=color:#2b91af>Protocol</span>.<span style=color:#2b91af>DEFAULT_HOST</span>),
</span></span><span style=display:flex><span>      conf.get(<span style=color:#a31515>&#34;spark.redis.auth&#34;</span>, <span style=color:#00f>null</span>)
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>/**
</span></span></span><span style=display:flex><span><span style=color:green>   *
</span></span></span><span style=display:flex><span><span style=color:green>   * @return use for JedisCluster or JedisPool
</span></span></span><span style=display:flex><span><span style=color:green>   */</span>
</span></span><span style=display:flex><span>  <span style=color:#00f>def</span> toSet()<span style=color:#00f>:</span> <span style=color:#2b91af>java.util.Set</span>[<span style=color:#2b91af>HostAndPort</span>] <span style=color:#00f>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#00f>val</span> nodes<span style=color:#00f>:</span> <span style=color:#2b91af>mutable.Set</span>[<span style=color:#2b91af>HostAndPort</span>] <span style=color:#00f>=</span> mutable.<span style=color:#2b91af>Set</span>()
</span></span><span style=display:flex><span>    <span style=color:#00f>for</span> (host_port <span style=color:#00f>&lt;-</span> clusterHosts.split(<span style=color:#a31515>&#34;,&#34;</span>)) {
</span></span><span style=display:flex><span>      <span style=color:#00f>val</span> hp <span style=color:#00f>=</span> host_port
</span></span><span style=display:flex><span>      print(hp)
</span></span><span style=display:flex><span>      nodes += <span style=color:#2b91af>HostAndPort</span>.from(host_port)
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>    nodes.asJava
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#00f>object</span> <span style=color:#2b91af>RedisClusterUtils</span> <span style=color:#00f>extends</span> <span style=color:#2b91af>Serializable</span> {
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  @transient <span style=color:#00f>private</span> <span style=color:#00f>lazy</span> <span style=color:#00f>val</span> pools<span style=color:#00f>:</span> <span style=color:#2b91af>ConcurrentHashMap</span>[<span style=color:#2b91af>RedisCluster</span>, <span style=color:#2b91af>JedisCluster</span>] <span style=color:#00f>=</span>
</span></span><span style=display:flex><span>    <span style=color:#00f>new</span> <span style=color:#2b91af>ConcurrentHashMap</span>[<span style=color:#2b91af>RedisCluster</span>, <span style=color:#2b91af>JedisCluster</span>]()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>/**
</span></span></span><span style=display:flex><span><span style=color:green>   * 获取一个JedisCluster
</span></span></span><span style=display:flex><span><span style=color:green>   * @param rc
</span></span></span><span style=display:flex><span><span style=color:green>   * @return
</span></span></span><span style=display:flex><span><span style=color:green>   */</span>
</span></span><span style=display:flex><span>  <span style=color:#00f>def</span> connect(rc<span style=color:#00f>:</span> <span style=color:#2b91af>RedisCluster</span>)<span style=color:#00f>:</span> <span style=color:#2b91af>JedisCluster</span> = {
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    pools.getOrElseUpdate(rc, {
</span></span><span style=display:flex><span>      <span style=color:#00f>val</span> poolConfig <span style=color:#00f>=</span> <span style=color:#00f>new</span> <span style=color:#2b91af>JedisPoolConfig</span>();
</span></span><span style=display:flex><span>      poolConfig.setMaxTotal(250)
</span></span><span style=display:flex><span>      poolConfig.setMaxIdle(32)
</span></span><span style=display:flex><span>      poolConfig.setTestOnBorrow(<span style=color:#00f>false</span>)
</span></span><span style=display:flex><span>      poolConfig.setTestOnReturn(<span style=color:#00f>false</span>)
</span></span><span style=display:flex><span>      poolConfig.setTestWhileIdle(<span style=color:#00f>false</span>)
</span></span><span style=display:flex><span>      poolConfig.setNumTestsPerEvictionRun(-1)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      <span style=color:#00f>val</span> jedisCluster <span style=color:#00f>=</span> <span style=color:#00f>new</span> <span style=color:#2b91af>JedisCluster</span>(rc.toSet(),
</span></span><span style=display:flex><span>        3000,
</span></span><span style=display:flex><span>        3000,
</span></span><span style=display:flex><span>        5,
</span></span><span style=display:flex><span>        rc.password,
</span></span><span style=display:flex><span>        poolConfig)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>      jedisCluster
</span></span><span style=display:flex><span>    })
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:green>/**
</span></span></span><span style=display:flex><span><span style=color:green>   * 查询币种对应汇率
</span></span></span><span style=display:flex><span><span style=color:green>   * @param jedisCluster 目标redis
</span></span></span><span style=display:flex><span><span style=color:green>   * @param ccyCd 币种代码
</span></span></span><span style=display:flex><span><span style=color:green>   * @return 折美元汇率
</span></span></span><span style=display:flex><span><span style=color:green>   */</span>
</span></span><span style=display:flex><span>  <span style=color:#00f>def</span> getCcyRatio(jedisCluster<span style=color:#00f>:</span> <span style=color:#2b91af>JedisCluster</span>, ccyCd<span style=color:#00f>:</span><span style=color:#2b91af>String</span>)<span style=color:#00f>:</span> <span style=color:#2b91af>Double</span> ={
</span></span><span style=display:flex><span>    <span style=color:#00f>val</span> res <span style=color:#00f>=</span> jedisCluster.get(<span style=color:#a31515>&#34;CCY:&#34;</span>+ccyCd)
</span></span><span style=display:flex><span>    res.split(<span style=color:#a31515>&#34;:&#34;</span>)(2).toDouble
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>参考<br><a href=https://spark.apache.org/docs/latest/streaming-programming-guide.html#design-patterns-for-using-foreachrdd>Design Patterns for using foreachRDD</a><br><a href=https://stackoverflow.com/questions/28006517/redis-on-sparktask-not-serializable>Redis on Spark:Task not serializable</a><br><a href=https://stackoverflow.com/questions/55190315/how-to-create-connections-to-a-datasource-in-spark-streaming-for-lookups>How to create connection(s) to a Datasource in Spark Streaming for Lookups</a></p><ul><li>DStream的RDD分区数是由topic分区数相同的.</li></ul><h2 id=最佳实践>最佳实践</h2></div><footer class=post-footer><div class=post-tags><a href=https://zhimoe.github.io/tags/code rel=tag title=code>#code#</a>
<a href=https://zhimoe.github.io/tags/spark rel=tag title=spark>#spark#</a></div><div class=post-nav><div class=article-copyright><div class=article-copyright-info><p><span>文章：</span><a href=https://zhimoe.github.io/post/spark-basic/> Spark Basic by zhimoe</a></p><p><span>声明： </span>本博客文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/ target=_blank style=text-decoration:underline>CC BY-NC-SA 3.0</a>许可协议，转载请注明出处！</p></div></div><div class=clear></div></div><div class=reward-qr-info><div>创作不易，打赏作者 ^_^</div><button id=rewardButton disable=enable onclick='var qr=document.getElementById("QR");qr.style.display==="none"?qr.style.display="block":qr.style.display="none"'>
<span>赏</span></button><div id=QR style=display:none><div id=wechat style=display:inline-block><img id=wechat_qr src=https://zhimoe.github.io/img/wechat-pay.jpg alt="WeChat Pay"><p>微信打赏</p></div><div id=alipay style=display:inline-block><img id=alipay_qr src=https://zhimoe.github.io/img/ali-pay.jpg alt=Alipay><p>支付宝打赏</p></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=https://zhimoe.github.io/post/java-threadpool-hierachy/ rel=next title="Java Thread Pool Hierachy"><i class="fa fa-chevron-left"></i> Java Thread Pool Hierachy</a></div><div class="post-nav-prev post-nav-item"><a href=https://zhimoe.github.io/post/how-test-void-method-with-parameter/ rel=prev title=单元测试如何Mock有参数的void方法>单元测试如何Mock有参数的void方法
<i class="fa fa-chevron-right"></i></a></div></div><div id=disqus_thread></div><script>var disqus_config=function(){this.page.url="https://zhimoe.github.io/post/spark-basic/",this.page.identifier="https://zhimoe.github.io/post/spark-basic/"};(function(){var e=document,t=e.createElement("script");t.src="https://zhimoe.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></footer></article></section></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
<span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
<span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id=sidebar class=sidebar><div class=sidebar-inner><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target=post-toc-wrap>文章目录</li><li class=sidebar-nav-overview data-target=site-overview>站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image src=https://zhimoe.github.io/img/avatar.png alt=zhimoe><p class=site-author-name itemprop=name>zhimoe</p><p class="site-description motion-element" itemprop=description>Captain your own Ship.</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=https://zhimoe.github.io/post/><span class=site-state-item-count>64</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=https://zhimoe.github.io/categories/><span class=site-state-item-count>4</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=https://zhimoe.github.io/tags/><span class=site-state-item-count>41</span>
<span class=site-state-item-name>标签</span></a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item><a href=https://github.com/zhimoe/ target=_blank title=GitHub><i class="fa fa-fw fa-github"></i>
GitHub</a></span>
<span class=links-of-author-item><a href=https://www.zhihu.com/question/21142149/answer/52383396 target=_blank title=ZhiHu><i class="fa fa-fw fa-globe"></i>
ZhiHu</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class=links-of-blogroll-title><i class="fa fa-fw fa-external-link"></i>
书签</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://learngitbranching.js.org/ title=可视化学习Git target=_blank>可视化学习Git</a></li><li class=links-of-blogroll-item><a href=https://gallerix.asia/ title=艺术绘画博物馆 target=_blank>艺术绘画博物馆</a></li><li class=links-of-blogroll-item><a href=https://cheats.rs/ title=RustCheatsheet target=_blank>RustCheatsheet</a></li><li class=links-of-blogroll-item><a href=https://developers.google.com/machine-learning/crash-course/ title=谷歌机器学习课程 target=_blank>谷歌机器学习课程</a></li></ul></div><div class="tagcloud-of-blogroll motion-element tagcloud-of-blogroll-inline"><div class=tagcloud-of-blogroll-title><i class="fa fa-fw fa-tags"></i>
标签云</div><ul class=tagcloud-of-blogroll-list><li class=tagcloud-of-blogroll-item><a href=https://zhimoe.github.io/tags/code>Code</a></li><li class=tagcloud-of-blogroll-item><a href=https://zhimoe.github.io/tags/java>Java</a></li><li class=tagcloud-of-blogroll-item><a href=https://zhimoe.github.io/tags/scala>Scala</a></li><li class=tagcloud-of-blogroll-item><a href=https://zhimoe.github.io/tags/python>Python</a></li><li class=tagcloud-of-blogroll-item><a href=https://zhimoe.github.io/tags/rust>Rust</a></li><li class=tagcloud-of-blogroll-item><a href=https://zhimoe.github.io/tags/spring>Spring</a></li><li class=tagcloud-of-blogroll-item><a href=https://zhimoe.github.io/tags/docker>Docker</a></li><li class=tagcloud-of-blogroll-item><a href=https://zhimoe.github.io/tags/git>Git</a></li><li class=tagcloud-of-blogroll-item><a href=https://zhimoe.github.io/tags/wsl>Wsl</a></li><li class=tagcloud-of-blogroll-item><a href=https://zhimoe.github.io/tags/aop>Aop</a></li></ul></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class=post-toc><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#引言>引言</a></li><li><a href=#hdfs>HDFS</a></li><li><a href=#yarn>YARN</a></li><li><a href=#spark基本概念>Spark基本概念</a></li><li><a href=#rdd>RDD</a></li><li><a href=#sparkstreaming>SparkStreaming</a></li><li><a href=#sparkstreamingkafka>SparkStreaming+Kafka</a></li><li><a href=#常见错误>常见错误</a><ul><li><a href=#数据库mysql-redis连接的可序列化问题>数据库(mysql redis)连接的可序列化问题</a></li></ul></li><li><a href=#最佳实践>最佳实践</a></li></ul></nav></div></div></section></div></aside></div></main><footer id=footer class=footer><div class=footer-inner><div class=copyright><span class=copyright-year>&copy; 2016 - 2023</span>
<span class=with-love><i class="fa fa-heart"></i></span>
<span class=copyright-author>zhimoe</span></div></div></footer><div class=back-to-top><i class="fa fa-arrow-up"></i>
<span id=scrollpercent><span>0</span>%</span></div></div><script type=text/javascript src=https://cdn.jsdelivr.net/npm/jquery@2.1.4/dist/jquery.min.js></script>
<script type=text/javascript src=https://zhimoe.github.io/js/search.js></script>
<script type=text/javascript src=https://zhimoe.github.io/js/affix.js></script>
<script type=text/javascript src=https://zhimoe.github.io/js/scrollspy.js></script>
<script type=text/javascript>function detectIE(){let e=window.navigator.userAgent,t=e.indexOf("MSIE "),n=e.indexOf("Trident/"),s=e.indexOf("Edge/");return t>0||n>0||s>0?-1:1}function getCntViewHeight(){let t=$("#content").height(),e=$(window).height();return t>e?t-e:$(document).height()-e}function getScrollbarWidth(){let e=$("<div />").addClass("scrollbar-measure").prependTo("body"),t=e[0],n=t.offsetWidth-t.clientWidth;return e.remove(),n}function registerBackTop(){let t=50,e=$(".back-to-top");$(window).on("scroll",function(){e.toggleClass("back-to-top-on",window.pageYOffset>t);let s=$(window).scrollTop(),o=getCntViewHeight(),i=s/o,n=Math.round(i*100),a=n>100?100:n;$("#scrollpercent>span").html(a)}),e.on("click",function(){$("html,body").animate({scrollTop:0,screenLeft:0},800)})}function initScrollSpy(){let e=".post-toc",s=$(e),t=".active-current";s.on("activate.bs.scrollspy",function(){let t=$(e+" .active").last();n(),t.addClass("active-current")}).on("clear.bs.scrollspy",n),$("body").scrollspy({target:e});function n(){$(e+" "+t).removeClass(t.substring(1))}}function initAffix(){let e=$(".header-inner").height(),t=parseInt($(".main").css("padding-bottom"),10),n=e+10;$(".sidebar-inner").affix({offset:{top:n,bottom:t}}),$(document).on("affixed.bs.affix",function(){updateTOCHeight(document.body.clientHeight-100)})}function initTOCDimension(){let e;$(window).on("resize",function(){e&&clearTimeout(e),e=setTimeout(function(){let e=document.body.clientHeight-100;updateTOCHeight(e)},0)}),updateTOCHeight(document.body.clientHeight-100);let t=getScrollbarWidth();$(".post-toc").css("width","calc(100% + "+t+"px)")}function updateTOCHeight(e){e=e||"auto",$(".post-toc").css("max-height",e)}$(function(){let t=$(".header-inner").height()+10;$("#sidebar").css({"margin-top":t}).show();let n=parseInt($("#sidebar").css("margin-top")),s=parseInt($(".sidebar-inner").css("height")),e=n+s,o=$(".content-wrap").height();o<e&&$(".content-wrap").css("min-height",e),$(".site-nav-toggle").on("click",function(){let e=$(".site-nav"),o=$(".toggle"),t="site-nav-on",i="toggle-close",n=e.hasClass(t),a=n?"slideUp":"slideDown",s=n?"removeClass":"addClass";e.stop()[a]("normal",function(){e[s](t),o[s](i)})}),registerBackTop(),initScrollSpy(),initAffix(),initTOCDimension(),$(".sidebar-nav-toc").click(function(){$(this).addClass("sidebar-nav-active"),$(this).next().removeClass("sidebar-nav-active"),$("."+$(this).next().attr("data-target")).toggle(500),$("."+$(this).attr("data-target")).toggle(500)}),$(".sidebar-nav-overview").click(function(){$(this).addClass("sidebar-nav-active"),$(this).prev().removeClass("sidebar-nav-active"),$("."+$(this).prev().attr("data-target")).toggle(500),$("."+$(this).attr("data-target")).toggle(500)})})</script><script src=https://cdn.jsdelivr.net/npm/imageviewer@1.1.0/dist/viewer.min.js></script>
<script type=text/javascript>$(function(){$(".post-body").viewer()})</script></body></html>